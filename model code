import numpy as np 
import pandas as pd
import glob
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, average_precision_score, roc_auc_score, classification_report
from sklearn.model_selection import StratifiedKFold
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import StratifiedShuffleSplit
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

pd.set_option('display.max_columns', None)

# Directory path containing the TSV files
directory_path = r"C:\Users\Windows 10-1703 ASUS\Desktop\university classes\structural bio\features_ring\\"

# Get a list of all TSV files in the directory
tsv_files = glob.glob(directory_path + "*.tsv")
# Create an empty list to store the dataframes
dataframes = []

# Iterate over the TSV files and read them into dataframes
for file in tsv_files:
    df = pd.read_csv(file, sep='\t')
    dataframes.append(df)
    
# Concatenate the dataframes into a single dataframe
combined_df = pd.concat(dataframes, ignore_index=True)


percentage of our target value missing
(np.sum(combined_df['Interaction'].isna())/combined_df['Interaction'].shape[0])*100



# Split the data into features and target 
features = combined_df.drop('Interaction', axis=1)
target = pd.DataFrame(combined_df['Interaction'])

num_col = []
for a in features.columns:
    if features[a].dtype == 'int64' or features[a].dtype == 'float64':
        num_col.append(a)
features[num_col]




features = features[num_col]
# Convert all categorical data to strings
features = features.astype({col: str for col in features.select_dtypes(include=['object', 'bool']).columns})
# Separate the numerical and non-numerical (categorical) columns
numerical_cols = features.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = features.select_dtypes(include=['object', 'bool']).columns
print(type(numerical_cols))

# Define preprocessing pipelines for both numerical and categorical data
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])                          

# Combine both preprocessing steps ,('cat', categorical_transformer, categorical_cols)]
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
        ])

# Fit and transform the data using just the preprocessor
preprocessed_num_data = preprocessor.fit_transform(features)

# Let's convert the result to a dense array if it's sparse                            + preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols).tolist()
if hasattr(preprocessed_num_data, "toarray"):
    preprocessed_num_data = preprocessed_num_data.toarray()

# Get numerical feature names from the preprocessor
num_feature_names = (preprocessor.named_transformers_['num'].named_steps['scaler'].get_feature_names_out(numerical_cols).tolist())

# Now create the DataFrame of just numerical features 
final_df = pd.DataFrame(preprocessed_num_data, columns=features[numerical_cols].columns)

# Print or return preprocessed dataframe(just numerical columns)
print(final_df)




# Preprocessing
le = preprocessing.LabelEncoder()
original_classes = target['Interaction'].unique()
target['Interaction'] = le.fit_transform(target['Interaction'])
X = final_df
y = target['Interaction']



over = SMOTE(sampling_strategy='not majority')
#under = RandomUnderSampler(sampling_strategy='majority')
steps = [('o', over)]   #('u', under)
pipeline = Pipeline(steps=steps)

X, y = pipeline.fit_resample(X, y)

# Define the splitter
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Get the indices for the sampled data
for train_indices, test_indices in splitter.split(X, y):
    #train_subset_size = 0.007  # Set the desired proportion for the train subset
    #test_subset_size = 0.003  # Set the desired proportion for the test subset

    # Select a subset of the train and test indices based on the desired proportion
    train_subset_indices = train_indices[:int(len(train_indices))]
    test_subset_indices = test_indices[:int(len(test_indices))]

    # Subset the original data to obtain the train and test subsets
    X_train = X.iloc[train_subset_indices]
    y_train = y.iloc[train_subset_indices]
    X_test = X.iloc[test_subset_indices]
    y_test = y.iloc[test_subset_indices]




from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1_l2
from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, roc_auc_score, classification_report
from sklearn.model_selection import StratifiedKFold
import numpy as np

def create_model(input_dim, num_classes):
    inputs = Input(shape=(input_dim,))
    
    x = Dense(512, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    
    x = Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    
    x = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    
    outputs = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['accuracy'])

    return model

# Cross-validation
kfold = StratifiedKFold(n_splits=2, shuffle=True)
matthews = []
balanced_accuracy = []
roc_scores = []

num_classes = len(np.unique(y_train))

for train, val in kfold.split(X_train, y_train):
    model = create_model(X_train.shape[1], num_classes)
    model.fit(X_train.iloc[train], y_train.iloc[train], epochs=20, verbose=0)
    y_pred = model.predict(X_train.iloc[val])
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_train_spec = y_train.iloc[val]
    # Performance metrics
    matthews.append(matthews_corrcoef(y_train_spec, y_pred_classes))
    balanced_accuracy.append(balanced_accuracy_score(y_train_spec, y_pred_classes))
    roc_scores.append(roc_auc_score(y_train_spec, y_pred, multi_class='ovr'))

# Calculate average scores
print('Average Matthews correlation coefficient:', np.mean(matthews))
print('Average Balanced Accuracy:', np.mean(balanced_accuracy))
print('Average ROC AUC Score:', np.mean(roc_scores))

# Final model evaluation on test data
y_test_pred = model.predict(X_test)
new_y_pred_classes = np.argmax(y_test_pred, axis=1)

print(classification_report(y_test, new_y_pred_classes, target_names= ['HBOND', 'IONIC', 'NI', 'PICATION', 'PIPISTACK', 'SSBOND','VDW' ]))





# Make predictions
new_y_pred = model.predict(X_test)
new_y_pred_classes = np.argmax(new_y_pred, axis=1)

# Calculate performance metrics
matthews = matthews_corrcoef(y_test, new_y_pred_classes)
balanced_accuracy = balanced_accuracy_score(y_test, new_y_pred_classes)
roc_score = roc_auc_score(y_test, new_y_pred, multi_class='ovr')

# Print the scores
print('Matthews correlation coefficient:', matthews)
print('Balanced Accuracy:', balanced_accuracy)
print('ROC AUC Score:', roc_score)

# Print the classification report
print('Classification Report:')
print(classification_report(le.inverse_transform(y_test), le.inverse_transform(new_y_pred_classes), target_names=['HBOND', 'IONIC', 'NI', 'PICATION', 'PIPISTACK', 'SSBOND','VDW' ]))
